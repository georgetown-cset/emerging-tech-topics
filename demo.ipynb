{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook demonstrates batch prediction on GCP for chip research classification.",
   "id": "efad63d515fcb7ad"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T21:35:57.129620Z",
     "start_time": "2024-12-16T21:35:55.599496Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import vertexai\n",
    "from bigframes import pandas as bpd\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from vertexai.batch_prediction import BatchPredictionJob\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Required\n",
    "PROJECT = os.getenv('PROJECT', 'gcp-cset-projects')\n",
    "\n",
    "# Model name must be one of \n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#models_that_support_batch_predictions\n",
    "MODEL = \"gemini-1.5-flash-002\"\n",
    "\n",
    "# Must be us-central1 at time of writing\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#request_a_batch_prediction_job_2\n",
    "LOCATION = 'us-central1'\n",
    "CENTRAL_DATASET = os.getenv(\"CENTRAL_DATASET\", \"tech_topics_demo_central\")\n",
    "\n",
    "# But our input data is in multi-region US\n",
    "US_DATASET = os.getenv(\"US_DATASET\", \"tech_topics_demo\")\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT)\n",
    "vertexai.init(project=PROJECT, location=LOCATION)\n",
    "\n",
    "central_dataset_reference = bigquery.DatasetReference(PROJECT, CENTRAL_DATASET)\n",
    "central_dataset = bigquery.Dataset(central_dataset_reference)\n",
    "central_dataset.location = 'us-central1'\n",
    "\n",
    "us_dataset_reference = bigquery.DatasetReference(PROJECT, US_DATASET)\n",
    "us_dataset = bigquery.Dataset(us_dataset_reference)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T21:33:31.083234Z",
     "start_time": "2024-12-16T21:33:30.029551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the datasets if they don't exist\n",
    "central_dataset = bq_client.create_dataset(central_dataset, exists_ok=True)\n",
    "us_dataset = bq_client.create_dataset(us_dataset, exists_ok=True)"
   ],
   "id": "e73a80d42bf518b5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll run just 1K inputs through the pipeline. Below we write these to a table called `corpus` in a dataset located in multi-region location `US`.",
   "id": "dd62b2249040633c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus_sql = Path('sql/chip_corpus.sql').read_text()\n",
    "corpus_sql += ' LIMIT 1000'\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    destination=f\"{PROJECT}.{US_DATASET}.corpus\",\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    ")\n",
    "\n",
    "query_job = bq_client.query(corpus_sql, job_config=job_config)\n",
    "query_job.result()"
   ],
   "id": "2bad02ee7fd02afa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The next few cells copy our demo dataset containing the `corpus` inputs from the multi-region `US` location to `us-central1`, using the BQ Data Transfer Service. This is necessary because (at time of writing) Batch Prediction on Vertex AI is only available in `us-central1`, while our input data is in `US`.\n",
    "\n",
    "First, we create a data transfer configuration."
   ],
   "id": "99278efc714c9f35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:00:26.569907Z",
     "start_time": "2024-12-16T22:00:25.152827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google.cloud import bigquery_datatransfer\n",
    "\n",
    "\n",
    "def create_transfer_config(source_dataset, destination_dataset, destination_location):\n",
    "    transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "    parent = f\"projects/{PROJECT}/locations/{destination_location}\"\n",
    "\n",
    "    # Transfer configuration for a dataset copy\n",
    "    transfer_config = bigquery_datatransfer.TransferConfig(\n",
    "        destination_dataset_id=destination_dataset,\n",
    "        display_name=f\"Transfer {source_dataset} to {destination_dataset}\",\n",
    "        data_source_id=\"cross_region_copy\",\n",
    "        params={\n",
    "            \"source_dataset_id\": source_dataset,\n",
    "            \"source_project_id\": PROJECT,\n",
    "        },\n",
    "        schedule_options={\n",
    "            \"disable_auto_scheduling\": True,\n",
    "        }\n",
    "    )\n",
    "    transfer_config = transfer_client.create_transfer_config(\n",
    "        parent=parent,\n",
    "        transfer_config=transfer_config,\n",
    "    )\n",
    "    return transfer_config\n",
    "\n",
    "\n",
    "# Create the transfer job config\n",
    "us_to_central_config = create_transfer_config(US_DATASET, CENTRAL_DATASET, LOCATION)"
   ],
   "id": "3a79f26faaca5eb8",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we trigger the transfer job.",
   "id": "288db6b168ff7fca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now(datetime.timezone.utc)\n",
    "\n",
    "transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "transfer_run = transfer_client.start_manual_transfer_runs({\n",
    "    \"parent\": us_to_central_config.name,\n",
    "    \"requested_run_time\": now,\n",
    "})\n",
    "transfer_run = transfer_run.runs[0]"
   ],
   "id": "d8cf03da20f16306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we wait for it to complete.",
   "id": "f7d867c9f833d536"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:08:12.409771Z",
     "start_time": "2024-12-16T22:08:12.404834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google.cloud.bigquery_datatransfer import TransferState\n",
    "import time\n",
    "\n",
    "while transfer_run.state not in (TransferState.SUCCEEDED, TransferState.FAILED):\n",
    "    time.sleep(5)\n",
    "    transfer_run = transfer_client.get_transfer_run({\n",
    "        \"name\": transfer_run.name,\n",
    "    })\n",
    "transfer_run.state"
   ],
   "id": "8de77ae9c9423401",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TransferState.SUCCEEDED: 4>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If our BQ inputs were already in `us-central1`, we could skip the above steps.\n",
    "\n",
    "We're now ready to prepare our inputs for the first batch prediction job, which generates a one-sentence summary for each title-abstract pair in the inputs.\n",
    "\n",
    "Below we load the SQL that'll create the batch input table for the summarization task."
   ],
   "id": "386dec2393db521d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def render_template(template_path, **kwargs):\n",
    "    return Template(Path(template_path).read_text()).render(**kwargs)\n",
    "\n",
    "\n",
    "summary_inputs_sql = render_template(\n",
    "    'sql/summary_inputs.sql',\n",
    "    dataset=CENTRAL_DATASET,\n",
    "    corpus='corpus'\n",
    ")"
   ],
   "id": "99a656a22d8c29a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We load the system instructions for summarization.",
   "id": "3f94a70618edec3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T23:52:26.488879Z",
     "start_time": "2024-12-16T23:52:26.485284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary_prompt = Path('prompts/chip-summarization.txt').read_text()\n",
    "print(summary_prompt)"
   ],
   "id": "b9da19cfb4d6ec20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in physics, chemistry, engineering, and materials science. Your task is to provide a concise summary of a research paper from its title and abstract text. (Or if the text isn't from a research paper, simply answer 'None'.) Your summary should be one sentence in length. Briefly mention the motivation for the work, and then focus on the research task(s) and research method(s) employed by the authors. Do not describe the purported benefits, advantages, importance, or impact of the research.\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define some UDFs we're using in the pipeline.",
   "id": "5727a4fad52b3e22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T23:54:20.839021Z",
     "start_time": "2024-12-16T23:54:17.726326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "udfs_sql = render_template('sql/udfs.sql', dataset=CENTRAL_DATASET)\n",
    "query_job = bq_client.query(udfs_sql, location=LOCATION)\n",
    "query_job.result()"
   ],
   "id": "74b905633d54f2ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x115c58590>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And we're ready to create the batch inputs.",
   "id": "c3dda3a11f5e948f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T23:59:22.432511Z",
     "start_time": "2024-12-16T23:59:19.275520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary_inputs_table = 'summary_inputs'\n",
    "summary_outputs_table = 'summary_outputs'\n",
    "\n",
    "\n",
    "def create_input_table(\n",
    "        query,\n",
    "        destination_table,\n",
    "        prompt,\n",
    "        temperature=0.5,\n",
    "        max_output_tokens=512\n",
    "):\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        destination=f\"{PROJECT}.{CENTRAL_DATASET}.{destination_table}\",\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        query_parameters=[\n",
    "            bigquery.ScalarQueryParameter(\"systemInstructions\", \"STRING\", prompt),\n",
    "            bigquery.ScalarQueryParameter(\"temperature\", \"FLOAT64\", temperature),\n",
    "            bigquery.ScalarQueryParameter(\"maxOutputTokens\", \"INT64\", max_output_tokens),\n",
    "        ]\n",
    "    )\n",
    "    job = bq_client.query(query, job_config=job_config, location=LOCATION)\n",
    "    print(f'Wrote {job.result().total_rows:,} rows to {destination_table}')\n",
    "\n",
    "\n",
    "create_input_table(\n",
    "    summary_inputs_sql,\n",
    "    summary_inputs_table,\n",
    "    summary_prompt\n",
    ")"
   ],
   "id": "ed52a6c0fc2aa00d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1,000 rows to summary_inputs\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below we start the batch summarization job.",
   "id": "c239b0c60347006d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T23:59:46.313731Z",
     "start_time": "2024-12-16T23:59:45.670687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_batch_job(input_table, output_table, model):\n",
    "    input_uri = f\"bq://{PROJECT}.{CENTRAL_DATASET}.{input_table}\"\n",
    "    output_uri = f\"bq://{PROJECT}.{CENTRAL_DATASET}.{output_table}\"\n",
    "    return BatchPredictionJob.submit(\n",
    "        source_model=model,\n",
    "        input_dataset=input_uri,\n",
    "        output_uri_prefix=output_uri,\n",
    "    )\n",
    "\n",
    "\n",
    "summary_job = create_batch_job(summary_inputs_table, summary_outputs_table, MODEL)"
   ],
   "id": "b589b0fdbf191406",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob created. Resource name: projects/855475113448/locations/us-central1/batchPredictionJobs/9023521745373495296\n",
      "To use this BatchPredictionJob in another session:\n",
      "job = batch_prediction.BatchPredictionJob('projects/855475113448/locations/us-central1/batchPredictionJobs/9023521745373495296')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/9023521745373495296?project=855475113448\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:02:44.280965Z",
     "start_time": "2024-12-16T23:59:55.534885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def await_job(job):\n",
    "    while not job.has_ended:\n",
    "        time.sleep(5)\n",
    "        job.refresh()\n",
    "    elapsed = datetime.datetime.now(tz=datetime.timezone.utc) - job.create_time\n",
    "    if job.has_succeeded:\n",
    "        print(f\"Job succeeded after {elapsed} h:m:s\")\n",
    "    else:\n",
    "        print(f\"Job failed: {job.error}\")\n",
    "\n",
    "\n",
    "await_job(summary_job)"
   ],
   "id": "409b01a4f28d1407",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job succeeded after 0:02:58.059158 h:m:s\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It doesn't take long for just 1K inputs. Now, using the one-sentence summaries, we prepare a classification task. Below is SQL for creating the batch input table for classification.",
   "id": "6a90a1b200f473c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "classify_inputs_sql = render_template(\n",
    "    'sql/classify_inputs.sql',\n",
    "    dataset=CENTRAL_DATASET,\n",
    "    summaries=summary_outputs_table,\n",
    ")"
   ],
   "id": "aac76d2b4df95607",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here are the system instructions for classification.",
   "id": "fc449e90b58840b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:03:26.003561Z",
     "start_time": "2024-12-17T00:03:25.999246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classify_prompt = Path('prompts/chip-classification.txt').read_text()\n",
    "print(classify_prompt)"
   ],
   "id": "6b7d57bfc3d62a19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in physics, chemistry, engineering, and materials science. Your task is to classify a research publication given a summary of its contents. Think carefully, and then decide whether the work focuses on the design and manufacturing of integrated circuits, or has applications to chip technology. If so answer YES; otherwise answer NO. Limit your answer to YES or NO.\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the above, we create the batch inputs for classification.",
   "id": "742aba8da70e942d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:03:31.568205Z",
     "start_time": "2024-12-17T00:03:28.838988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classify_inputs_table = 'classify_inputs'\n",
    "classify_outputs_table = 'classify_outputs'\n",
    "\n",
    "create_input_table(\n",
    "    classify_inputs_sql,\n",
    "    classify_inputs_table,\n",
    "    classify_prompt,\n",
    "    temperature=0.0,\n",
    "    max_output_tokens=5\n",
    ")"
   ],
   "id": "7c4c017202cf3014",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 793 rows to classify_inputs\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And we start the batch classification job.",
   "id": "9094bebbdcbfa957"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:06:51.516426Z",
     "start_time": "2024-12-17T00:03:46.910083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classify_job = create_batch_job(classify_inputs_table, classify_outputs_table, MODEL)\n",
    "await_job(classify_job)"
   ],
   "id": "c2e3827157491e54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob created. Resource name: projects/855475113448/locations/us-central1/batchPredictionJobs/4534558816791953408\n",
      "To use this BatchPredictionJob in another session:\n",
      "job = batch_prediction.BatchPredictionJob('projects/855475113448/locations/us-central1/batchPredictionJobs/4534558816791953408')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/4534558816791953408?project=855475113448\n",
      "Job succeeded after 0:02:54.205242 h:m:s\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The SQL below parses the output from the classification task.",
   "id": "4c9254e79e40ee25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels_sql = render_template(\n",
    "    'sql/labels.sql',\n",
    "    dataset=CENTRAL_DATASET,\n",
    "    labels=classify_outputs_table,\n",
    "    corpus='corpus',\n",
    ")"
   ],
   "id": "75dc38a71dedd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We write the resulting labels to a `labels` table.",
   "id": "28587ce5ae0fd891"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:08:24.502080Z",
     "start_time": "2024-12-17T00:08:21.072992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels_table = 'labels'\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    destination=f\"{PROJECT}.{CENTRAL_DATASET}.{labels_table}\",\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    ")\n",
    "query_job = bq_client.query(labels_sql, job_config=job_config, location=LOCATION)\n",
    "print(f'Wrote {query_job.result().total_rows:,} rows to {labels_table}')"
   ],
   "id": "1af9f1a62821a38d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1,000 rows to labels\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The outputs from each step include `usageMetadata` that we can aggregate to estimate costs. For 1K inputs, pipeline costs were about 0.6 cents.",
   "id": "4c03ddcffb4ff342"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:14:39.052841Z",
     "start_time": "2024-12-17T00:14:29.985397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "usage_sql = render_template(\n",
    "    'sql/usage.sql',\n",
    "    dataset=CENTRAL_DATASET,\n",
    "    summaries=summary_outputs_table,\n",
    "    labels=classify_outputs_table,\n",
    ")\n",
    "\n",
    "bpd.close_session()\n",
    "bpd.options.bigquery.location = LOCATION\n",
    "\n",
    "bpd.read_gbq(usage_sql)"
   ],
   "id": "a9a0f384a585b89f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Query job 970cf784-1328-43c4-93e5-f736fb8a6ffc is DONE. 28.7 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=gcp-cset-projects&j=bq:us-central1:970cf784-1328-43c4-93e5-f736fb8a6ffc&page=queryresults\">Open Job</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Query job 9c329c2f-0b5a-47f3-8d95-5228490f3d71 is DONE. 40 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=gcp-cset-projects&j=bq:us-central1:9c329c2f-0b5a-47f3-8d95-5228490f3d71&page=queryresults\">Open Job</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "   prompt  output  prompt_cost  output_cost  total_cost\n",
       "0  428550   48503     0.004018     0.001819    0.005837\n",
       "\n",
       "[1 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>output</th>\n",
       "      <th>prompt_cost</th>\n",
       "      <th>output_cost</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>428550</td>\n",
       "      <td>48503</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.005837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5 columns</p>\n",
       "</div>[1 rows x 5 columns in total]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wrap up by summarizing our predictions.",
   "id": "dba53634e5e61b8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:16:14.708381Z",
     "start_time": "2024-12-17T00:16:07.454724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bpd.read_gbq(f\"\"\"\\\n",
    "select\n",
    "  label,\n",
    "  count(*) as count\n",
    "from {CENTRAL_DATASET}.labels\n",
    "group by label\n",
    "\"\"\")"
   ],
   "id": "d1b09e049d84ae7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Query job 4e644b79-0491-42f5-865d-184d823fb2e7 is DONE. 1.0 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=gcp-cset-projects&j=bq:us-central1:4e644b79-0491-42f5-865d-184d823fb2e7&page=queryresults\">Open Job</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Query job 419644db-0e2c-4ce2-8818-22badd1168d4 is DONE. 18 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=gcp-cset-projects&j=bq:us-central1:419644db-0e2c-4ce2-8818-22badd1168d4&page=queryresults\">Open Job</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "   label  count\n",
       "0   True     35\n",
       "1  False    965\n",
       "\n",
       "[2 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 2 columns</p>\n",
       "</div>[2 rows x 2 columns in total]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 131
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
